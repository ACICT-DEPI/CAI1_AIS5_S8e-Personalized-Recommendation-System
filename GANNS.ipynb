{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[45843,45595] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Create models\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m build_discriminator(num_classes \u001b[38;5;241m+\u001b[39m num_classes_tmdb)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Compile the discriminator\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mbuild_generator\u001b[1;34m(latent_dim, num_classes)\u001b[0m\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39mlatent_dim))\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# For imdbId\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes_tmdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# For tmdbId\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\medoo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\medoo\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\medoo\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[45843,45595] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.read_csv('dataset/links.csv')\n",
    "# Encode categorical data\n",
    "label_encoder_imdb = LabelEncoder()\n",
    "label_encoder_tmdb = LabelEncoder()\n",
    "\n",
    "df['imdbId'] = label_encoder_imdb.fit_transform(df['imdbId'])\n",
    "df['tmdbId'] = label_encoder_tmdb.fit_transform(df['tmdbId'])\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df[['movieId', 'imdbId', 'tmdbId']].values\n",
    "num_classes = max(X[:, 1]) + 1  # for imdbId\n",
    "num_classes_tmdb = max(X[:, 2]) + 1  # for tmdbId\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator(latent_dim, num_classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # For imdbId\n",
    "    model.add(layers.Dense(num_classes_tmdb, activation='softmax'))  # For tmdbId\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator(num_classes):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(num_classes + num_classes_tmdb,)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary output\n",
    "    return model\n",
    "\n",
    "# Set parameters\n",
    "latent_dim = 10\n",
    "epochs = 10000\n",
    "batch_size = 8\n",
    "\n",
    "# Create models\n",
    "generator = build_generator(latent_dim, num_classes)\n",
    "discriminator = build_discriminator(num_classes + num_classes_tmdb)\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "discriminator.trainable = False  # Freeze the discriminator when training the generator\n",
    "\n",
    "# Connect the generator and discriminator\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "generated_data = generator(gan_input)\n",
    "\n",
    "# Flatten generated data to feed into the discriminator\n",
    "flattened_data = layers.Flatten()(generated_data)\n",
    "gan_output = discriminator(flattened_data)\n",
    "\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training function\n",
    "def train_gan(gan, generator, discriminator, X_train, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        # Generate random noise\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "        \n",
    "        # Generate new data\n",
    "        generated_data = generator.predict(noise)\n",
    "\n",
    "        # Get a random set of real data\n",
    "        real_data = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]\n",
    "        \n",
    "        # Combine real and fake data\n",
    "        combined_data = np.concatenate([real_data, generated_data])\n",
    "        \n",
    "        # Create labels for real (1) and fake (0) data\n",
    "        labels = np.array([1] * batch_size + [0] * batch_size)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss = discriminator.train_on_batch(combined_data, labels)\n",
    "\n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "        valid_labels = np.array([1] * batch_size)  # Labels for the generator to \"fool\" the discriminator\n",
    "        \n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "        # Print the progress\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch: {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}')\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(gan, generator, discriminator, X_train, epochs, batch_size)\n",
    "\n",
    "# Generate new samples\n",
    "num_samples = 5  # Number of samples to generate\n",
    "noise = np.random.normal(0, 1, size=(num_samples, latent_dim))\n",
    "generated_samples = generator.predict(noise)\n",
    "\n",
    "# Convert generated samples back to original format\n",
    "imdb_ids_generated = np.argmax(generated_samples[:, :num_classes], axis=1)\n",
    "tmdb_ids_generated = np.argmax(generated_samples[:, num_classes:], axis=1)\n",
    "\n",
    "# Map back to original ids\n",
    "imdb_ids_original = label_encoder_imdb.inverse_transform(imdb_ids_generated)\n",
    "tmdb_ids_original = label_encoder_tmdb.inverse_transform(tmdb_ids_generated)\n",
    "\n",
    "# Display generated samples\n",
    "generated_df = pd.DataFrame({\n",
    "    'movieId': range(len(imdb_ids_original)),\n",
    "    'imdbId': imdb_ids_original,\n",
    "    'tmdbId': tmdb_ids_original\n",
    "})\n",
    "\n",
    "print(\"Generated Samples:\")\n",
    "print(generated_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
